{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4710882,"sourceType":"datasetVersion","datasetId":2725217},{"sourceId":4835254,"sourceType":"datasetVersion","datasetId":2794811}],"dockerImageVersionId":30370,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U kaleido","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n![](https://i.imgur.com/SLT8Zfk.jpg)\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>1 |</span></b> <b>BACKGROUND</b></div>\n\n\nIn this section we will outline what customer feedback is, why it is an important part of any business, not only for financial companies, but in general. Show some examples, which should show that it can take some time to manually read and analyse what each consumer complaint is about. How can we utilise consumer feedback to streamline the process of consumer-company interaction (the need)\n\n<br>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>CONSUMER FEEDBACK</span></b></p></div>\n    \nLet's point our some key points about consumer feedback:\n    \n\n<b><span style='color:#FFC300'>Consumer feedback</span></b> is an important part of day to day financial business operations\nCompanies offering products must be able to know what their consumers think of their products\nEg. positive & negative feedback & can be obtained from a number of sources (eg. twitter)\nIn this case, we obtain data from a <a href=\"https://www.consumerfinance.gov/data-research/consumer-complaints/\"><b>database</b></a>, which registers consumers feedback of financial products\nCustomers have specific <b>issue</b> on a number of <b>topics</b> they want want the company to address\nThe form of consumer communication with the financial institution is via the <b>web</b> (as will be shown later)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>USEFULNESS OF CONSUMER FEEDBACK</span></b></p></div>\n\n**<span style='color:#FFC300'>Can your customers tell you something important?</span>** | **[Source](https://www.startquestion.com/blog/7-reasons-why-customer-feedback-is-important-to-your-business/)**\n\n> If you run your own business, I know you do your best to please your customers <br>\n> Satisfy their needs, and keep them loyal to your brand.  <br>\n> But how can you be sure that your efforts bring desired results?  <br>\n> If you do not try to find out what your clients think about your service <br>\n> You will never be able to give them the best customer experience. <br>\n> **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.7\">Their opinions</mark>** about their experience with your brand are helpful information<br> \n> That you can **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.7\">use to adjust your business to fit their needs more accurately</mark>**\n\n- The source clearly outlines that consumer feedback is quite critical for any business\n- **<span style='color:#FFC300'>Consumer feedback</span>** in our problem is related to a consumer having an `issue` with a particualr financial `product` or alike","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424;text-align:center'>CONSUMER FEEDBACK EXAMPLES</span></b></p></div>\n\nLet's look at a couple of examples of a consumer's addresses to a company:\n\nProduct: **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Credit reporting</mark>** | Issue: **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">Incorrect information on credit report</mark>**\n\n> After looking at my credit report I saw a collection account that does not belong to me. I am not allowed to dispute this information online on Experian or over the phone making it impossible for me. This false information is ruining my credit and knowing full well this people did not do their job and allow people to just post false accounts on my report. They need to delete this information immediately and do a proper investigation as this information is not mine. ' \n\nProduct: **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Credit card</mark>** | Issue: **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">Credit line increase/decrease</mark>**\n\n> \"XXXX i receive an email from citibank regarding my XXXX credit card. It was an offer to request a credit increase and it clearly stated that there would be NO Credit bureau inquiry made. I clicked on the link in the email and entered the requested information. a couple of days later I received an alert from my credit bureau monitoring service that a hard inquiry was done. Upon looking at the report it showed Citibank credit cards making a hard credit inquiry which was completely opposite of what their email stated. I called citi and they confimred that the email stated there would be no creidt inquiry done however they said that the request was made on a different citibank credit card which is why the hard inquiry was made. I explained to the rep I clicked on the link they provided and if was for a different account of mine it was not my issue but theirs and they need to remove the inquiry. They told me to send a letter to their credit dispute department explaining it. I sent the letter after waiting more than a month I received a blunt statement stating the it was a valid credit request and they will not remove the inquiry from my credit bureau. Citi performed bait and switch by offering a no inquiry credit request and then doing a hard inquiry which has negatively affected my credit score. I asked to remove it and received a generic letter stating they would not with no number to contact the department that sent the letter when i called the main customer service number they said that department dosent talk to customers and there was nothing else they can do. This has negatively affected my credit score and will remain on my credit report for 2 years because citi 's False advertising. and then their lack of fixing their error \"\n\n<br>\n\nAfter reading this long complaint: \n\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white; width: auto;\">\n<ul>\n<li>It should become apparent that <b>manual evaluation</b> of each consumer issue can can a while to process and is very inefficient</li>\n<li>For a timely & helpful consumer response, the <b>relevant problem</b> not only must be processed in a timely manner, but passed on to <b>a specific expert that has experience dealing with the particular issue</b>\n</li>\n</ul></div>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>2 |</span></b> <b>NOTEBOOK WORKFLOW</b></div>\n\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>NOTEBOOK AIM</span></b></p></div>\n    \n- In this notebook, we'll be utilising <b><span style='color:#FFC300'>machine learning</span></b>, to create a model(s) that will be able to classify the type of complaint (as we did above) (by both <b>product</b> & <b>issue</b>)\n<li>Such a model can be useful for a company to quickly understand the type of complaint & appoint a financial expert that will be able to solve the problem\n\n<br>\n   \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>AUTOMATED TICKET CLASSIFICATION SYSTEM</span></b></p></div>\n    \nOur approach will include **<span style='color:#FFC300'>separate models</span>**, that will be in charge of classifying data on different subsets of data\n\n- **<span style='color:#FFC300'>M1</span>** will be classifying a  **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">product</mark>** based on the customer's input complaint (`text`) (eg. Credit Reporting)\n- <b><span style='color:#FFC300'>M2</span></b> will be in charge of classifying the particular <b><mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">issue</mark></b> to which the complaint belongs to (<b>text</b>)","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>3 |</span></b> <b>DATASET PREPROCESSING</b></div>\n\nIn this section, we will dive in to the dataset, making some slight adjustments; loading the data, looking at missing data, looking at the features & make some slight adustments\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CONSUMER COMPLAINT DATABASE</span></b></p></div>\n\n- Download full [**Dataset**](https://www.consumerfinance.gov/data-research/consumer-complaints/) \n- Complaints that the Consumer Financial Protection Bureau (CFPB) sends to companies for response are published in the Consumer Complaint Database after the company responds\n- Confirming a commercial relationship with the consumer, or after 15 days, whichever comes first","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>LOAD DATASET</span></b></p></div>\n\nWe start off by loading the dataset (we are loading the dataset without any missing data in `consumer complaint narrative` (which is the complaint)","metadata":{}},{"cell_type":"code","source":"%%time\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('/kaggle/input/complaintsfull/main.csv',low_memory=False)\ndf = df.drop(['Unnamed: 0'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:59:52.129712Z","iopub.execute_input":"2023-07-28T08:59:52.130471Z","iopub.status.idle":"2023-07-28T09:00:13.647254Z","shell.execute_reply.started":"2023-07-28T08:59:52.130387Z","shell.execute_reply":"2023-07-28T09:00:13.645965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TARGET LABELS</span></b></p></div>\n\nA quick glimps into the dataset gives us the view of the features that will be of interest to us in this **NLP problem**\n\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n    \n- <b>Product</b> (Type of financial product)\n- <b>Sub-product</b>  (A more detailed subset of product)\n- <b>Issue</b> (What was the problem)\n- <b>Sub-issue</b> (A more detailed subset of product)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>MISSING DATA</span></b></p></div>\n\nVisualise missing data in the dataset, looks like we have quite a bit overall & some in target variables (`Sub-Product` & `Sub-Issue`)","metadata":{}},{"cell_type":"code","source":"import missingno as ms\nms.matrix(df)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:00:13.653676Z","iopub.execute_input":"2023-07-28T09:00:13.654069Z","iopub.status.idle":"2023-07-28T09:00:24.596081Z","shell.execute_reply.started":"2023-07-28T09:00:13.654016Z","shell.execute_reply":"2023-07-28T09:00:24.595027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li> We have quite a bit of missing data, we have already removed rows, which have missing data in our text target (consumer complaint narrative)</li>\n    <li> And quite a heavy dataset, let's utilise only the relevant data for our problem (by the end of this section) which should reduce the number of rows in our data significatntly\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>FEATURE DESCRIPTION</span></b></p></div>\n\nBrief summary of what each feature represents in our dataset","metadata":{}},{"cell_type":"code","source":"print('Dataset Features')\ndf.columns","metadata":{"papermill":{"duration":0.029471,"end_time":"2023-01-30T05:45:37.983353","exception":false,"start_time":"2023-01-30T05:45:37.953882","status":"completed"},"tags":[],"editable":false,"execution":{"iopub.status.busy":"2023-07-28T09:00:24.597834Z","iopub.execute_input":"2023-07-28T09:00:24.598532Z","iopub.status.idle":"2023-07-28T09:00:24.610874Z","shell.execute_reply.started":"2023-07-28T09:00:24.598492Z","shell.execute_reply":"2023-07-28T09:00:24.609948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n    \n<ul>\n    <li><b><span style='color:#FFC300'>Date received</span></b> - When the complaint was addressed</li>\n    <li><b><span style='color:#FFC300'>Product</span></b> - Complaint Type</li>\n    <li><b><span style='color:#FFC300'>Issue</span></b> - Brief summary of the issue</li>\n    <li><b><span style='color:#FFC300'>Consumer complaint narrative</span></b> - What the customer wrote (documents)</li>\n    <li><b><span style='color:#FFC300'>Company public response</span></b> - How did the company respond</li>\n    <li><b><span style='color:#FFC300'>State</span></b> - State in which the complaint was made</li>\n    <li><b><span style='color:#FFC300'>Submitted</span></b> - Form of complaint</li>\n    <li><b><span style='color:#FFC300'>Customer disputed?</span></b> - Did the customer dispute the response</li>\n\n</ul>\n</div>\n\n<br>","metadata":{"papermill":{"duration":0.018437,"end_time":"2023-01-30T05:45:38.018825","exception":false,"start_time":"2023-01-30T05:45:38.000388","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>ADDING DATETIME FEATURES</span></b></p></div>\n\nLets add time-series based features, normalise column and column subset names & remove some column subset data for our target variable\n\n- We have two timeline features, `Date received` & `Date sent to company`\n- Lets extract additional features which can be useful for EDA","metadata":{}},{"cell_type":"code","source":"def object_to_datetime_features(df,column):\n\n    df[column] = df[column].astype('datetime64[ns]')\n    df['Year'] = df[column].dt.year\n    df['Month'] = df[column].dt.month\n    df['Day'] = df[column].dt.day\n    df['DoW'] = df[column].dt.dayofweek\n    df['DoW'] = df['DoW'].replace({0:'Monday',1:'Tuesday',2:'Wednesday',\n                                   3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'})\n    return df\n\ndf = object_to_datetime_features(df,'Date received')\ndf.columns","metadata":{"papermill":{"duration":1.014619,"end_time":"2023-01-30T05:45:39.084092","exception":false,"start_time":"2023-01-30T05:45:38.069473","status":"completed"},"tags":[],"editable":false,"execution":{"iopub.status.busy":"2023-07-28T09:00:24.616412Z","iopub.execute_input":"2023-07-28T09:00:24.617722Z","iopub.status.idle":"2023-07-28T09:00:25.741737Z","shell.execute_reply.started":"2023-07-28T09:00:24.617679Z","shell.execute_reply":"2023-07-28T09:00:25.740585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>NORMALISE COLUMN NAMES</span></b></p></div>\n\nLets convert all column names to a lower register","metadata":{}},{"cell_type":"code","source":"# lower the register of columns\n\ndef normalise_column_names(df):\n    \n    normalised_features = [i.lower() for i in list(df.columns)]\n    df.columns = normalised_features\n    return df\n\ndf = normalise_column_names(df)","metadata":{"papermill":{"duration":0.026791,"end_time":"2023-01-30T05:45:39.128746","exception":false,"start_time":"2023-01-30T05:45:39.101955","status":"completed"},"tags":[],"editable":false,"execution":{"iopub.status.busy":"2023-07-28T09:00:25.743241Z","iopub.execute_input":"2023-07-28T09:00:25.743709Z","iopub.status.idle":"2023-07-28T09:00:25.750214Z","shell.execute_reply.started":"2023-07-28T09:00:25.743669Z","shell.execute_reply":"2023-07-28T09:00:25.749057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>NORMALISE THE SUBSET NAMES</span></b></p></div>\n\nLets convert all subset feature names into a lower register as well","metadata":{}},{"cell_type":"code","source":"# show the names of each subset\n\ndef show_subset_names(df,column):    \n    return df[column].value_counts().index\n\nshow_subset_names(df,'product')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:00:25.751995Z","iopub.execute_input":"2023-07-28T09:00:25.752401Z","iopub.status.idle":"2023-07-28T09:00:25.885714Z","shell.execute_reply.started":"2023-07-28T09:00:25.752361Z","shell.execute_reply":"2023-07-28T09:00:25.884528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalise_subset_names(df,column):\n    subset_names = list(df[column].value_counts().index)\n    norm_subset_names = [i.lower() for i in subset_names]\n    dict_replace = dict(zip(subset_names,norm_subset_names))\n    df[column] = df[column].replace(dict_replace)    \n    return df\n\ndf = normalise_subset_names(df,'product')\nshow_subset_names(df,'product')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:00:25.887408Z","iopub.execute_input":"2023-07-28T09:00:25.887805Z","iopub.status.idle":"2023-07-28T09:00:27.233005Z","shell.execute_reply.started":"2023-07-28T09:00:25.887756Z","shell.execute_reply":"2023-07-28T09:00:27.232017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>KEEP ONLY SPECIFIC SUBSETS</span></b></p></div>\n\nLet's keep only specific subsets of data in the **product** column","metadata":{}},{"cell_type":"code","source":"# keep only specific subset in a feature\n\ndef keep_subset(df,column,lst):\n    \n    all_features = list(df[column].value_counts().index)\n    keep_features = lst\n    \n    # subset data\n    subset_data = dict(tuple(df.groupby(column)))\n    subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n    \n    # dictionary with only selected keys\n    filtered_data = subset_data_filter(subset_data,lst)\n    filtered_df = pd.concat(filtered_data.values())\n    filtered_df.reset_index(drop=True,inplace=True)\n    return filtered_df\n    \n# remove specific subset from feature\n\ndef remove_subset(df,column,lst):\n    \n    all_features = list(df[column].value_counts().index)\n    keep_features = lst\n    \n    # subset data\n    subset_data = dict(tuple(df.groupby(column)))\n    set_all_features = set(all_features)\n    set_keep_features = set(lst)\n    \n    # features of dictionary which should remain\n    remaining_features = set_all_features - set_keep_features\n \n    subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n    filtered_data = subset_data_filter(subset_data,remaining_features)\n    filtered_df = pd.concat(filtered_data.values())\n    filtered_df.reset_index(drop=True,inplace=True)\n    return filtered_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-28T09:00:27.234499Z","iopub.execute_input":"2023-07-28T09:00:27.23517Z","iopub.status.idle":"2023-07-28T09:00:27.246767Z","shell.execute_reply.started":"2023-07-28T09:00:27.23513Z","shell.execute_reply":"2023-07-28T09:00:27.245677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lst_keep = ['credit reporting', 'debt collection', 'mortgage', 'credit card',\n            'bank account or service', 'consumer Loan', 'student loan',\n            'payday loan', 'prepaid card', 'money transfers',\n            'other financial service', 'virtual currency']\n\ndf = keep_subset(df,'product',lst_keep)\n# sdf = remove_subset(df,'product',lst_remove)\ndf['product'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:00:27.248354Z","iopub.execute_input":"2023-07-28T09:00:27.248823Z","iopub.status.idle":"2023-07-28T09:00:29.554218Z","shell.execute_reply.started":"2023-07-28T09:00:27.248783Z","shell.execute_reply":"2023-07-28T09:00:29.553048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>NORMALISE THE SUBSET NAMES</span></b></p></div>\n\n- Let's choose only a specific subset of time series data\n- We can note that data beyond 2017 has only three of the available subsets","metadata":{}},{"cell_type":"code","source":"df['year'].value_counts()","metadata":{"papermill":{"duration":0.03037,"end_time":"2023-01-30T05:45:42.023975","exception":false,"start_time":"2023-01-30T05:45:41.993605","status":"completed"},"tags":[],"editable":false,"execution":{"iopub.status.busy":"2023-07-28T09:00:29.557328Z","iopub.execute_input":"2023-07-28T09:00:29.557734Z","iopub.status.idle":"2023-07-28T09:00:29.593484Z","shell.execute_reply.started":"2023-07-28T09:00:29.557693Z","shell.execute_reply":"2023-07-28T09:00:29.592223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ldf = df.groupby('product').count()['day'].to_frame().sort_values(ascending=False,by='day')\n\nldf.style\\\n    .bar(align='mid',\n         color=['#d65f5f','#F1A424'])","metadata":{"execution":{"iopub.status.busy":"2023-07-28T09:00:29.59711Z","iopub.execute_input":"2023-07-28T09:00:29.597629Z","iopub.status.idle":"2023-07-28T09:00:30.699951Z","shell.execute_reply.started":"2023-07-28T09:00:29.597589Z","shell.execute_reply":"2023-07-28T09:00:30.6988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have 184 thousand samples to go by, which should still be sufficient for our task","metadata":{"papermill":{"duration":0.017929,"end_time":"2023-01-30T05:45:42.958071","exception":false,"start_time":"2023-01-30T05:45:42.940142","status":"completed"},"tags":[],"editable":false}},{"cell_type":"code","source":"df = df[df['year'].isin([2015,2016,2017])]\nprint(f'final shape: {df.shape}')","metadata":{"papermill":{"duration":0.102509,"end_time":"2023-01-30T05:45:43.077745","exception":false,"start_time":"2023-01-30T05:45:42.975236","status":"completed"},"tags":[],"editable":false,"execution":{"iopub.status.busy":"2023-07-28T09:00:30.704734Z","iopub.execute_input":"2023-07-28T09:00:30.707308Z","iopub.status.idle":"2023-07-28T09:00:30.838443Z","shell.execute_reply.started":"2023-07-28T09:00:30.707258Z","shell.execute_reply":"2023-07-28T09:00:30.837364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>4 |</span></b> <b>TARGET DISTRIBUTION</b></div>\n\nIn this section, we will look at distributions of our target features, we will be creating two models \n\nTwo of which are **target variables** in our problem:\n> - Product Value Distribution (target variable for **<span style='color:#FFC300'>M1</span>**)\n> - Consumer Complaint Summary (What was the issue, in a few words) (target variable for **<span style='color:#FFC300'>M2</span>**)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>PRODUCT</span></b></p></div>\n\nLet's plot the grouped subset distribution for feature `Product` ","metadata":{}},{"cell_type":"code","source":"fig = px.bar((df['product']\n              .value_counts(ascending=False)\n              .to_frame()),\n             x='product',\n             template='plotly_white',\n             title='Product Subset Distribution')\n\nfig.update_traces(marker_line_color='#F1A424',\n                  marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\n\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-28T09:00:30.845846Z","iopub.execute_input":"2023-07-28T09:00:30.846191Z","iopub.status.idle":"2023-07-28T09:00:32.370456Z","shell.execute_reply.started":"2023-07-28T09:00:30.846159Z","shell.execute_reply":"2023-07-28T09:00:32.369094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>ISSUES</span></b></p></div>\n\n- Let's review the **summaries of each complaint**, which is stored in `Issue` (target variable for **<span style='color:#FFC300'>M2</span>**)\n- The data has been divided into **<span style='color:#FFC300'>88 different issues</span>**, which is a tad more categories than our target variable <code>Product</code> for **<span style='color:#FFC300'>M1</span>**","metadata":{}},{"cell_type":"code","source":"ldf = df['issue'].value_counts(ascending=True).to_frame()\n\nfig = px.bar(ldf,x='issue',\n             template='plotly_white',\n             title='Issue of Complaint',\n             text_auto=True)\n\nfig.update_layout(height=1500)\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\n\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can note that we have some target class inbalance, in both product and issue features, \n- Lets stick **stratification**, we need to make sure each class is represented in both datasets","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>5 |</span></b> <b>EXPLORATORY DATA ANALYSIS</b></div>\n\nThe dataset contains a lot of **categorical** which can be grouped & analysed:\n- Consumer Complaint Method (How was the complaint made?)\n- Company to which the consumer complained (To which company was the complaint made)\n- Consumer Complaint Timeline (When were the complaints made?)\n- Consumer Complaint Timeline Group Trends (grouping data; are there any trends in the timeline)\n- Consumer Address State (In which state was the complaint made?)\n- Company Response to Consumer Complaints (What was the response to the complaint?)\n- Consumer Response to company response (Did the consumer dispuse the response?)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CONSUMER COMPLAINT METHOD</span></b></p></div>\n\n\n- All customers addresses for which we have text data are submitted via the **web**\n- This implies that all other forms of text data of complaints were not registered in the data for other forms of complaint submission","metadata":{}},{"cell_type":"code","source":"df['submitted via'].value_counts(ascending=True)","metadata":{"papermill":{"duration":0.055643,"end_time":"2023-01-30T05:45:44.629814","exception":false,"start_time":"2023-01-30T05:45:44.574171","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>FINANCIAL COMPANY</span></b></p></div>\n\n\n","metadata":{}},{"cell_type":"code","source":"# plot subset value counts \n\ndef plot_subset_counts(df,column,orient='h',top=None):\n    \n    ldf = df[column].value_counts(ascending=False).to_frame()\n    ldf.columns = ['values']\n    \n    if(top):\n        ldf = ldf[:top]\n    \n    if(orient is 'h'):\n        fig = px.bar(data_frame=ldf,\n                     x = ldf.index,\n                     y = 'values',\n                     template='plotly_white',\n                     title='Subset Value-Counts')\n\n    elif('v'):\n\n        fig = px.bar(data_frame=ldf,\n                             y = ldf.index,\n                             x = 'values',\n                             template='plotly_white',\n                             title='Subset Value-Counts')\n        \n    fig.update_layout(height=400)\n    fig.update_traces(marker_line_color='white',\n                      marker_line_width=0.5,\n                      marker={'color':'#F1A424'},\n                      width=0.75)\n    \n    fig.show(\"png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_subset_counts(df,'company',orient='v',top=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>COMPANY RESPONSE TO COMPLAINT</span></b></p></div>\n\n\nLet's look at the data about what the company decided to do about the registered complaint","metadata":{}},{"cell_type":"code","source":"ldf = df['company public response'].value_counts(ascending=False).to_frame()\n\nldf.style\\\n    .bar(align='mid',\n         color=['#3b3745','#F1A424'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>Most of the consumer complaints were addressed in private as opposed to public</li>\n</ul>\n</div>","metadata":{"papermill":{"duration":0.025683,"end_time":"2023-01-30T05:45:44.907937","exception":false,"start_time":"2023-01-30T05:45:44.882254","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CONSUMER COMPLAINT TIMELINE</span></b></p></div>\n\n\n- Let's look at the **weekly** (using `resample`) complaint addresses (let's see if there are some trends in the time series data)","metadata":{}},{"cell_type":"code","source":"# Daily Complaints\ncomplaints = df.copy()\ncomplaints_daily = complaints.groupby(['date received']).agg(\"count\")[[\"product\"]] # daily addresses\n\n# Sample weekly\ncomplaints_weekly = complaints_daily.reset_index()\ncomplaints_weekly = complaints_weekly.resample('W', on='date received').sum() # weekly addresses\n\nfig = px.line(complaints_weekly,complaints_weekly.index,y=\"product\",\n              template=\"plotly_white\",title=\"Weekly Complaints\",height=400)\nfig.update_traces(line_color='#F1A424')\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>We can observe an increasing trend in complaints registed (this could be because users simply regitered the complaints more)</li>\n    <li>After April 2nd, 2017, there is a rapid decline in registered complaints\n    <li>Some interesting peaks with an unusually high number of registed complaints occured in 2017 (January,September)</li>\n    <li>We can note that the pandemic had also an effect on the number of complaints registed</li>\n</ul>\n</div>","metadata":{"papermill":{"duration":0.01815,"end_time":"2023-01-30T05:45:45.374768","exception":false,"start_time":"2023-01-30T05:45:45.356618","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CONSUMER COMPLAINT TIMELINE TRENDS</span></b></p></div>\n\n\nLet's group all complaint data into groups for **Day of the month** (DoM), **time of the year** (ToY) & **day of the week** (DoW)","metadata":{}},{"cell_type":"code","source":"fig = px.bar(df['day'].value_counts(ascending=True).to_frame(),y='day',\n             template='plotly_white',height=300,\n             title='Day of the Month Complaint Trends')\nfig.update_xaxes(tickvals = [i for i in range(0,32,1)])\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.update_traces(textfont_size=12, textangle=0, \n                  textposition=\"outside\", cliponaxis=False)\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(df['month'].value_counts(ascending=False).to_frame(),y='month',\n             template='plotly_white',height=300,\n             title='Month of the Year Complaint Trends')\nfig.update_xaxes(tickvals = [i for i in range(0,13,1)])\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# By DoW\nfig = px.bar(df['dow'].value_counts(ascending=False).to_frame(),y='dow',\n             template='plotly_white',height=300,\n             title='Day of the Week Complaint Trends')\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\nfig.show(\"png\")","metadata":{"papermill":{"duration":0.079572,"end_time":"2023-01-30T05:45:45.704081","exception":false,"start_time":"2023-01-30T05:45:45.624509","status":"completed"},"tags":[],"editable":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>Day of the month seems to be quite a cyclic trend, with lower numbers closer to the weekends</li>\n    <li><b>July</b>, <b>August</b> & <b>September</b> are associated with increased complaints, <b>December</b>, <b>January</b> & <b>February</b> associated with lower number of complaints</li>\n    <li><b>Novermber</b>, <b>December</b>, <b>January</b> & <b>February</b> don't have adequate data</li>\n    <li><b>Tuesdays</b> & <b>Wednesdays</b> tend to be the most common day a consumer will write a complaint</li>\n    <li>Consumers don't tend to write complaints on weekends (<b>Saturday</b>,<b>Sunday</b>)</li>\n</ul>\n</div>","metadata":{"papermill":{"duration":0.018476,"end_time":"2023-01-30T05:45:45.741804","exception":false,"start_time":"2023-01-30T05:45:45.723328","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CONSUMER COMPLAINT ORIGINS</span></b></p></div>\n\nLet's investigate the distribution from which geographical state the complaint was made from","metadata":{}},{"cell_type":"code","source":"ldf = df['state'].value_counts(ascending=True).to_frame()[50:]\n\nfig = px.bar(ldf,x='state',template='plotly_white',\n       title='State of Complaint',height=400)\n\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=1,\n                  marker={'color':'#F1A424'},width=0.4)\nfig.show(\"png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>Most of the consumer complaints are from California, Florida, Texas, Georgia and New York</li>\n</ul>\n</div>","metadata":{"papermill":{"duration":0.018564,"end_time":"2023-01-30T05:45:45.918837","exception":false,"start_time":"2023-01-30T05:45:45.900273","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>COMPARING COMPANY RESPONSE TO CONSIMER DISPUTES</span></b></p></div>\n\n\nLastly, we can combine the last two sections and see for each `Product`, how many disputes there have been each month","metadata":{}},{"cell_type":"code","source":"disputed = df[['product','consumer disputed?','month']]\n\nfig = px.histogram(disputed, y='product', \n                   color='consumer disputed?',\n                   template='plotly_white',\n                   height = 700,\n                   barmode='group',\n                   color_discrete_sequence=['#F1A424','#3b3745'],\n                   facet_col_wrap=3,\n                   facet_col='month')\n\nfig.update_layout(showlegend=False)\nfig.update_layout(barmode=\"overlay\")\nfig.update_traces(opacity=0.5)\nfig.show(\"png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li><b>Morgage</b> & <b>Debt Collection</b> tend to be quite often disputed all year round</li>\n    <li>September & October have the highest disputed cases for <b>Morgage</b> & <b>Debt Collection</b> </li>\n    <li>July & August have an unusually high ammount of <b>credit reporting</b> complaints</li>\n    \n</ul>\n</div>","metadata":{"papermill":{"duration":0.039776,"end_time":"2023-01-30T05:45:46.981427","exception":false,"start_time":"2023-01-30T05:45:46.941651","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>6 |</span></b> <b>PREPARING DATA FOR CLASSIFICATION</b></div>\n\n- We've done some exploratory data analysis & understand our problem target variables a little better, let's focus on preparing the data for machine learning \n- In total, we have **<span style='color:#FFC300'>1159430 complaints</span>**, although not evenly distributed as we saw in **<span style='color:#FFC300'>Section 3</span>**\n- Our smallest class (**<span style='color:#FFC300'>virtual currency</span>**) has only **<span style='color:#FFC300'>16 issues</span>** (which is very little data)","metadata":{}},{"cell_type":"code","source":"df['product'].value_counts(ascending=False).to_frame().sum()","metadata":{"papermill":{"duration":0.059242,"end_time":"2023-01-30T05:45:47.15802","exception":false,"start_time":"2023-01-30T05:45:47.098778","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['product'].value_counts(ascending=False).to_frame().tail(3)","metadata":{"papermill":{"duration":0.05968,"end_time":"2023-01-30T05:45:47.255899","exception":false,"start_time":"2023-01-30T05:45:47.196219","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>REVIEW LOW COUNT CLASS</span></b></p></div>\n\nThere doesn't seem to be any error associated with labelling, so let's not remove this subset\n\n> Circle is a Boston-based financial services company that uses blockchain technology for its peer-to-peer payments and cryptocurrency-related products.\n\nDespite wanting to utilise **Stratification**, it seems like we may not have enough data for the model to be able to classify complaints with little data available","metadata":{}},{"cell_type":"code","source":"print('Sample from virtual currency:')\nvc = dict(tuple(df.groupby(by='product')))['virtual currency']\nvc.iloc[[0]]['consumer complaint narrative'].values[0]","metadata":{"papermill":{"duration":0.115052,"end_time":"2023-01-30T05:45:47.498844","exception":false,"start_time":"2023-01-30T05:45:47.383792","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>PRODUCT SUBSET AMBIGUITY</span></b></p></div>\n\n\n- We have a feature `Credit reporting, credit repair services, or other personal consumer reports`, it seems like this feature is not quite sorted \n- We already have a separate subgroup for **Credit reporting**, but not for **cedit repair services** or **other consumer reports**\n- There is the possibility that this subgroup will contain complaints of other subgroups, which would affect the model accuracy\n- Lets remove the subset: **Credit reporting, credit repair services, or other personal consumer reports** for the time being \n\nSome of the approaches we could take are:\n- Try to sort the data by keywords\n","metadata":{}},{"cell_type":"code","source":"# When we have a feature that contains subsets, we can remove unwanted subsets\n        \ndef remove_subset(df,feature,lst_groups):\n    \n    ndf = df.copy()\n\n    # Let's down sample all classes with frequencies above 4k\n    group = dict(tuple(ndf.groupby(by=feature)))\n    subset_group = list(group.keys()) # subsets in feature\n    \n    # Check if features exist in columns\n    if(set(lst_groups).issubset(subset_group)):\n\n        # remove unwanted subset\n        for k in lst_groups:\n            group.pop(k, None)\n\n        df = pd.concat(list(group.values()))\n        df.reset_index(inplace=True,drop=True)\n\n        return df    ","metadata":{"papermill":{"duration":0.05072,"end_time":"2023-01-30T05:45:47.667027","exception":false,"start_time":"2023-01-30T05:45:47.616307","status":"completed"},"tags":[],"editable":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We are down to 611,233 complaints (about have of what we had)\n- Let's also confirm that our function **remove_subset** works correctly","metadata":{"papermill":{"duration":0.039601,"end_time":"2023-01-30T05:45:47.745737","exception":false,"start_time":"2023-01-30T05:45:47.706136","status":"completed"},"tags":[],"editable":false}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>PRODUCT SUBSET AMBIGUITY</span></b></p></div>\n\nWe saw that we have quite a bit of data available\n- Most of which are in particular classes (eg. credit collection ,debt reporting & mortgage)\n- Let's limit our data to **4000 text samples** from each class using function `downsample_features`","metadata":{}},{"cell_type":"code","source":"# Downsample selected \n\ndef downsample_subset(df,feature,lst_groups,samples=4000):\n\n    ndf = df.copy()\n\n    # Let's down sample all classes with frequencies above 4k\n    group = dict(tuple(ndf.groupby(by=feature)))\n    subset_group = list(group.keys())\n\n    # Check if features exist in columns\n    if(set(lst_groups).issubset(subset_group)):\n\n        dict_downsamples = {}\n        for feature in lst_groups:\n            dict_downsamples[feature] = group[feature].sample(samples)\n\n        # remove old data\n        for k in lst_groups:\n            group.pop(k, None)\n\n        # read them back\n        group.update(dict_downsamples)\n\n        df = pd.concat(list(group.values()))\n        df.reset_index(inplace=True,drop=True)\n\n        return df\n    \n    else:\n        print('feature not found in dataframe')\n        ","metadata":{"papermill":{"duration":0.050737,"end_time":"2023-01-30T05:45:47.916627","exception":false,"start_time":"2023-01-30T05:45:47.86589","status":"completed"},"tags":[],"editable":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select subset features which have more than 4000 samples\nsubset_list = list(df['product'].value_counts()[df['product'].value_counts().values > 4000].index)\ndf = downsample_subset(df,'product',subset_list,samples=4000)","metadata":{"papermill":{"duration":0.277214,"end_time":"2023-01-30T05:45:48.233598","exception":false,"start_time":"2023-01-30T05:45:47.956384","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Let's check if our function `downsample_subset` works correctly","metadata":{"papermill":{"duration":0.041997,"end_time":"2023-01-30T05:45:48.315323","exception":false,"start_time":"2023-01-30T05:45:48.273326","status":"completed"},"tags":[],"editable":false}},{"cell_type":"code","source":"df['product'].value_counts()","metadata":{"papermill":{"duration":0.054015,"end_time":"2023-01-30T05:45:48.409987","exception":false,"start_time":"2023-01-30T05:45:48.355972","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TRAIN-TEST SPLITTING</span></b></p></div>\n\n- Next, as per standard requirements, we need to be able to validate the model after training\n- We will split the data into two groups, training & validation subsets with a validation size of **0.2**\n- **<span style='color:#FFC300'>Stratification</span>** will also be applied to both groups, in order to guarantee all classes in both subgroups","metadata":{}},{"cell_type":"code","source":"# Select only relevant data\ndf_data = df[['consumer complaint narrative','product']]\ndf_data.columns = ['text','label']\ndf_data.head()","metadata":{"papermill":{"duration":0.065639,"end_time":"2023-01-30T05:45:48.594712","exception":false,"start_time":"2023-01-30T05:45:48.529073","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\n\ntrain_files,test_files, train_labels, test_labels = tts(df_data['text'],\n                                                        df_data['label'],\n                                                        test_size=0.1,\n                                                        random_state=32,\n                                                        stratify=df_data['label'])\n\ntrain_files = pd.DataFrame(train_files)\ntest_files = pd.DataFrame(test_files)\ntrain_files['label'] = train_labels\ntest_files['label'] = test_labels","metadata":{"papermill":{"duration":0.212176,"end_time":"2023-01-30T05:45:48.924117","exception":false,"start_time":"2023-01-30T05:45:48.711941","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(train_files))\nprint('Training Data',train_files.shape)\nprint('Validation Data',test_files.shape)","metadata":{"papermill":{"duration":0.048564,"end_time":"2023-01-30T05:45:49.012483","exception":false,"start_time":"2023-01-30T05:45:48.963919","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\ntrain_values = train_files['label'].value_counts()\ntest_values = test_files['label'].value_counts()\nvisual = pd.concat([train_values,test_values],axis=1)\nvisual = visual.T\nvisual.index = ['train','test']\n\nfig = px.bar(visual,template='plotly_white',\n       barmode='group',text_auto=True,height=300,\n       title='Train/Test Split Distribution')\n\nfig.show(\"png\")","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.143658,"end_time":"2023-01-30T05:45:49.195013","exception":false,"start_time":"2023-01-30T05:45:49.051355","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/F3MTDMN.jpg)\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>7 |</span></b> <b>M1 LINEAR BASELINE MODEL</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>GENERATING HUGGINGFACE DATASET</span></b></p></div>\n\n- We have a daframe containing **text** & **label** data for both training & validation datasets\n- We'll use HF's more intuitive to use `Dataset` class (which allows us to convert between types very easily)","metadata":{}},{"cell_type":"code","source":"train_files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\ntransformers.logging.set_verbosity_error()\nimport warnings; warnings.filterwarnings('ignore')\nimport os; os.environ['WANDB_DISABLED'] = 'true'\nfrom datasets import Dataset,Features,Value,ClassLabel, DatasetDict \n\ntraindts = Dataset.from_pandas(train_files)\ntraindts = traindts.class_encode_column(\"label\")\ntestdts = Dataset.from_pandas(test_files)\ntestdts = testdts.class_encode_column(\"label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pandas indicies not reset ie. __index_level_0__ additional column is added, resetting index\ncorpus = DatasetDict({\"train\" : traindts , \n                      \"validation\" : testdts })\ncorpus['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TOKENISATION OF TEXT</span></b></p></div>\n\n- Time to tokenise the **text** data\n- We'll be using the `AutoTokenizer` class from a pretrained model `distilbert-base-uncased` in order to generate **subword** tokens","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load parameters of the tokeniser\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Tokenisation function\ndef tokenise(batch):\n    return tokenizer(batch[\"text\"], \n                     padding=True, \n                     truncation=True)\n\n# apply to the entire dataset (train,test and validation dataset)\ncorpus_tokenised = corpus.map(tokenise, \n                              batched=True, \n                              batch_size=None)\n\nprint(corpus_tokenised[\"train\"].column_names)","metadata":{"papermill":{"duration":0.048186,"end_time":"2023-01-30T05:45:49.526855","exception":false,"start_time":"2023-01-30T05:45:49.478669","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>LOADING PRESET MODEL</span></b></p></div>\n\n- HuggingFace allows us to load a variety of pretrained models:\n    - Let's utlilise the **<span style='color:#F1A424'>[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)</span>** model\n    \n    \n- The model was trained to predict **[mask] values**\n    - Given an input sequence (the above link shows an example)\n    \n    \n- Let's use it to extract the **<span style='color:#F1A424'>last hidden state</span>** of each input sequence\n    - Use it to train a more tradition machine learning model (baseline **<span style='color:#F1A424'>M1</span>** model)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch\n\n# load a pretrained transformer model\nmodel_ckpt = \"distilbert-base-uncased\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# move model to device\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)","metadata":{"papermill":{"duration":0.048448,"end_time":"2023-01-30T05:45:49.691737","exception":false,"start_time":"2023-01-30T05:45:49.643289","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>EXTRACTION HIDDEN STATE FUNCTION</span></b></p></div>\n\n- For each tokenised input `text`, we can utilise the loaded model & extract the last hidden state that can be used as features for machine learning models\n- The same strategy was applied in notebook **[Twitter Emotion Classification](https://www.kaggle.com/code/shtrausslearning/twitter-emotion-classification#5-|-Training-a-Text-Classifier)**","metadata":{}},{"cell_type":"code","source":"# Function used to store last_hidden_state data of distilbert-base-uncased\ndef extract_hidden_states(batch):\n    \n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n    \n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n        \n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}","metadata":{"papermill":{"duration":0.049903,"end_time":"2023-01-30T05:45:49.861252","exception":false,"start_time":"2023-01-30T05:45:49.811349","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>CHANGE DATA TO TENSORS</span></b></p></div>\n\nBefore training using the `pytorch` model, we need to set the corresponing format using `set_format`","metadata":{}},{"cell_type":"code","source":"# Change Data to Torch tensor\ncorpus_tokenised.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\ncorpus_tokenised","metadata":{"papermill":{"duration":0.047453,"end_time":"2023-01-30T05:45:50.027102","exception":false,"start_time":"2023-01-30T05:45:49.979649","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>PREDICT AND EXTRACT LAST HIDDEN STATE</span></b></p></div>\n\nUisng `map` we can apply the function `extract_hidden_states` to both **training** & **validation** datasets","metadata":{}},{"cell_type":"code","source":"# Extract last hidden states (faster w/ GPU)\ncorpus_hidden = corpus_tokenised.map(extract_hidden_states, \n                                     batched=True,\n                                     batch_size=32)\ncorpus_hidden[\"train\"].column_names","metadata":{"papermill":{"duration":0.047904,"end_time":"2023-01-30T05:45:50.191548","exception":false,"start_time":"2023-01-30T05:45:50.143644","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Empty cache\ntorch.cuda.empty_cache()","metadata":{"papermill":{"duration":0.047857,"end_time":"2023-01-30T05:45:50.277449","exception":false,"start_time":"2023-01-30T05:45:50.229592","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The extracted hidden state corpus `corpus_hidden` has been uploaded to **[dataset](https://www.kaggle.com/datasets/shtrausslearning/hiddenstatedata)**","metadata":{}},{"cell_type":"code","source":"# Save our data\ncorpus_hidden.set_format(type=\"pandas\")\n\n# Add label data to dataframe\ndef label_int2str(row):\n    return corpus[\"train\"].features[\"label\"].int2str(row)\n\nldf = corpus_hidden[\"train\"][:]\nldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str)\nldf.to_pickle('training.df')\n\nldf = corpus_hidden[\"validation\"][:]\nldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str)\nldf.to_pickle('validation.df')","metadata":{"papermill":{"duration":0.049036,"end_time":"2023-01-30T05:45:50.444204","exception":false,"start_time":"2023-01-30T05:45:50.395168","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/\n# !ls /kaggle/input/hiddenstatedata/","metadata":{"papermill":{"duration":0.05032,"end_time":"2023-01-30T05:45:50.533564","exception":false,"start_time":"2023-01-30T05:45:50.483244","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>DEFINING HIDDEN STATE DATASET</span></b></p></div>\n\n- Having extracted the last hidden state data from model **distilbert-base-uncased**, let's define the training data\n- The process is quite long so we'll load the data saved in the last section from **training.csv** & **validation.csv**","metadata":{}},{"cell_type":"code","source":"# reload saved data\nimport pandas as pd\nimport pickle\n\n# Load hidden state data\n# training = pd.read_pickle('/kaggle/input/hiddenstatedata/training.df')\n# validation = pd.read_pickle('/kaggle/input/hiddenstatedata/validation.df')\ntraining = pd.read_pickle('training.df')\nvalidation = pd.read_pickle('validation.df')\ntraining.head()\n\nlabels = training[['label','label_name']]","metadata":{"papermill":{"duration":0.049131,"end_time":"2023-01-30T05:45:50.702068","exception":false,"start_time":"2023-01-30T05:45:50.652937","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = []\nfor i in labels.label.unique():\n    label.append(labels[labels['label'] == i].iloc[[0]]['label_name'].values[0])\n    \nlabel","metadata":{"papermill":{"duration":0.050378,"end_time":"2023-01-30T05:45:50.791256","exception":false,"start_time":"2023-01-30T05:45:50.740878","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define our training & validation datasets\n\nimport numpy as np\nX_train = np.stack(training['hidden_state'])\nX_valid = np.stack(validation[\"hidden_state\"])\ny_train = np.array(training[\"label\"])\ny_valid = np.array(validation[\"label\"])\nprint(f'Training Dataset: {X_train.shape}')\nprint(f'Validation Dataset {X_valid.shape}')","metadata":{"papermill":{"duration":0.049763,"end_time":"2023-01-30T05:45:50.880391","exception":false,"start_time":"2023-01-30T05:45:50.830628","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TRAIN BASELINE MODEL</span></b></p></div>\n\nLet's start with something quite simplistic, `LogisticRegression` often work quite well","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.linear_model import LogisticRegression as LR\n\n# We increase `max_iter` to guarantee convergence\nlr_clf = LR(max_iter = 2000)\nlr_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\ny_preds_train = lr_clf.predict(X_train)\ny_preds_valid = lr_clf.predict(X_valid)\nprint('LogisticRegression:')\nprint(f'training accuracy: {round(lr_clf.score(X_train, y_train),3)}')\nprint(f'validation accuracy: {round(lr_clf.score(X_valid, y_valid),3)}')","metadata":{"papermill":{"duration":0.048347,"end_time":"2023-01-30T05:45:51.136296","exception":false,"start_time":"2023-01-30T05:45:51.087949","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save sklearn model\nimport joblib\n\nfilename = 'classifier.joblib.pkl'\n_ = joblib.dump(lr_clf, filename, compress=9)\n\n# load sklearn model\n# lr_clf = joblib.load('/kaggle/input/hiddenstatedata/' + filename)\n# lr_clf","metadata":{"papermill":{"duration":0.047858,"end_time":"2023-01-30T05:45:51.222593","exception":false,"start_time":"2023-01-30T05:45:51.174735","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_model, y_true, labels):\n    cm = confusion_matrix(y_true,y_model,normalize='true')\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm.round(2).copy(), display_labels=labels)\n    disp.plot(ax=ax, colorbar=False)\n    plt.title(\"Confusion matrix\")\n    plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n    plt.tight_layout()\n    plt.show()","metadata":{"papermill":{"duration":0.048079,"end_time":"2023-01-30T05:45:51.308997","exception":false,"start_time":"2023-01-30T05:45:51.260918","status":"completed"},"tags":[],"editable":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(training.label_name.value_counts().index)\n\n# Validation Dataset Confusion Matrix\nplot_confusion_matrix(y_preds_valid, y_valid, labels)","metadata":{"papermill":{"duration":0.048139,"end_time":"2023-01-30T05:45:51.395591","exception":false,"start_time":"2023-01-30T05:45:51.347452","status":"completed"},"tags":[],"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>Compressed distilbert embedding features work quite well in this this problem</li>\n    <li>Looks like we have quite a good model to begin with, scoring a validation accuracy of 0.77</li>\n    <li>We can note that the model <b>payday loan</b> & <b>virtual currency</b> are very pooly predicted subsets</li>\n    <li><b>other financial services</b> is predicted quite well (which is surprising because for the transformer model, we have the opposite)\n</ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3b3745\"><b><span style='color:#F1A424'>8 |</span></b> <b>M1 FINE-TUNED TRANSFORMER MODEL</b></div>\n\n- With the fine-tune approach, we do not use the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.7\">hidden states</mark>** as fixed features\n- Instead, we train them from a given model state\n- This requires the **<span style='color:#FFC300'>classification head</span>** to be differentiable (neural network for classification)","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>LOADING A PRETRAINED MODEL</span></b></p></div>\n\n- We'll load the same **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.7\">DistilBERT</mark>** model using `model_ckpt` **\"distilbert-base-uncased\"** \n- This time however we will be loading `AutoModelForSequenceClassification` (we used `AutoModel` when we extracted embedding features)\n- `AutoModelForSequenceClassification` model has a **<span style='color:#FFC300'>classification head</span>** on top of the pretrained model outputs\n- We only need to specify the **<span style='color:#FFC300'>number of labels</span>** the model has to predict `num_labels`","metadata":{}},{"cell_type":"code","source":"# Empty cache\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change Data to Torch tensor\ncorpus_tokenised.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\ncorpus_tokenised","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ckpt = \"distilbert-base-uncased\"\n\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, \n                          num_labels=len(labels))\n         .to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>DEFINING THE PERFORMANCE METRICS</span></b></p></div>\n\n- We'll monitor the `F1 score`  & `accuracy`, the function is required to be passed in the `Trainer` class","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TRAINING PARAMETERS</span></b></p></div>\n\n- Next we need to define the model **training parameters**, which can be done using `TrainingArguments`\n- Let's train the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.7\">DistilBERT</mark>** model for **3 iterations** with a **learning rate of 2e-5** and a **batch size of 64**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbs = 16 # batch size\nmodel_name = f\"{model_ckpt}-finetuned-financial\"\nlabels = corpus_tokenised[\"train\"].features[\"label\"].names\n\n# Training Arguments\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=3,             # number of training epochs\n                                  learning_rate=2e-5,             # model learning rate\n                                  per_device_train_batch_size=bs, # batch size\n                                  per_device_eval_batch_size=bs,  # batch size\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False, \n                                  report_to=\"none\",\n                                  push_to_hub=False,\n                                  log_level=\"error\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>DEFINE A TRAINER</span></b></p></div>\n\nThe `Trainer` requires inputs of a model, model arguments, metrics, the datasets (train,validation) & the tokeniser","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(model=model,                                 # Model\n                  args=training_args,                          # Training arguments (above)\n                  compute_metrics=compute_metrics,             # Computational Metrics\n                  train_dataset=corpus_tokenised[\"train\"],     # Training Dataset   \n                  eval_dataset=corpus_tokenised[\"validation\"], # Evaluation Dataset\n                  tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>TRAIN MODEL</span></b></p></div>\n\nLet's finally fine-tune our transform model to fit our classification problem ","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Train & save model\ntrainer.train()\ntrainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n# load from previously saved model\n# classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-financial\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>INFERENCE</span></b></p></div>\n\nLet's utilise our fine-tuned transformer model for inference on the validation dataset","metadata":{}},{"cell_type":"code","source":"# Predict on Validation Dataset\npred_output = trainer.predict(corpus_tokenised[\"validation\"])\npred_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Output Predition: {pred_output.predictions.shape}')\nprint(pred_output.predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Decode the predictions greedily using argmax (highest value of all classes)\ny_preds = np.argmax(pred_output.predictions,axis=1)\nprint(f'Output Prediction:{y_preds.shape}')\nprint(f'Predictions: {y_preds}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation \nplot_confusion_matrix(y_preds,y_valid,labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<ul>\n    <li>Fine-tuning a pretrained-transformed works sufficiently better than our baseline approach</li>\n    <li><b>virtual currency</b> is also not predicted very well (same as the linear model)</li>\n    <li><b>payday loan</b> is predicted well but <b>other financial services</b> poorly\n</ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\" background-color:#F1A424; padding: 13px 13px; border-radius: 8px; color: white; text-align:center;\">\n<h4 style=\"color:white;\">If you liked the notebook, upvote</h4>\nIf you have any questions (suggestions), please ask (write) below\n</div>","metadata":{}}]}